{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja efektów uczenia konwolucyjnych sieci neuronowych\n",
    "\n",
    "Często słyszy się, że modele uczenia głębokiego są jak „czarne skrzynki”: uczą się reprezentacji, których ekstrakcja jest trudna do wykonania i trudno jest je przedstawić w formie czytelnej z punktu widzenia człowieka. Stwierdzenie to dla niektórych modeli uczenia głębokiego można uznać za częściowo prawdziwe, ale z pewnością nie jest ono prawdziwe w przypadku pracy z konwolucyjnymi sieciami neuronowymi. Reprezentacje uczone przez konwolucyjne sieci neuronowe można z łatwością przedstawić w sposób graficzny. Wynika to w dużej mierze z tego, że są to reprezentacje koncepcji graficznych. Od 2013 r. rozwinięto wiele technik wizualizacji i interpretowania takich reprezentacji. Nie będę opisywał ich wszystkich, ale przedstawię Ci trzy najprostsze rozwiązania tego typu:\n",
    "\n",
    "* Wizualizacja pośrednich danych wyjściowych (pośrednich aktywacji) — technika przydatna do zrozumienia tego, jak kolejne warstwy sieci konwolucyjnej przekształcają dane wejściowe i tego, co robią poszczególne filtry sieci.\n",
    "* Wizualizacja filtrów sieci konwolucyjnej — technika umożliwiająca dokładne zrozumienie graficznego wzorca lub graficznej koncepcji, na którą reaguje każdy z filtrów sieci konwolucyjnej.\n",
    "* Wizualizacja map ciepła aktywacji klas obrazu — technika przydatna do określenia części obrazu zidentyfikowanych jako należące do danej klasy — pozwala zlokalizować obiekty widoczne na obrazach.\n",
    "\n",
    "Działanie pierwszej techniki — wizualizacji aktywacji — sprawdzimy na przykładzie małej sieci konwolucyjnej wytrenowanej od podstaw podczas podejmowania próby rozwiązania klasyfikacji obrazów psów i kotów w podrozdziale 5.2. Działanie dwóch kolejnych technik sprawdzimy na modelu VGG16 z podrozdziału 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja pośrednich aktywacji\n",
    "\n",
    "Wizualizacja pośrednich aktywacji polega na wyświetlaniu map cech, które są generowane przez różne warstwy konwolucyjne i łączące sieci na podstawie określonych danych wejściowych (dane wyjściowe warstwy są często określane mianem aktywacji, wynikiem zwracanym przez funkcję aktywacji). Umożliwia ona podgląd tego, jak dane wejściowe są rozkładane przez różne filtry wyuczone przez sieć. Chcemy dokonać wizualizacji map cech w przestrzeni trzech wymiarów: wysokości, szerokości i głębi (kanały). Każdy kanał koduje względnie niezależne cechy, a więc w celu wykonania poprawnej wizualizacji map tych cech należy przedstawić zawartość każdego kanału w formie niezależnego dwuwymiarowego obrazu. Zacznijmy od załadowania modelu zapisanego podczas rozwiązywania problemu z podrozdziału 5.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('cats_and_dogs_small_2.h5')\n",
    "model.summary()  # W celu przypomnienia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz uzyskamy obraz wejściowy — zdjęcie kota niewchodzące w skład treningowego zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'\n",
    "# Ustawka Technologiczna, 14.06.2019 -> przetestujcie również na zdjęciu cat.1700.jpg\n",
    "\n",
    "\n",
    "# Wstępnie przetwarzamy obraz tak, aby uzyskał formę czterowymiarowego tensora.\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Pamiętaj o tym, że model był trenowany \n",
    "# na danych przetworzonych właśnie w taki sposób.\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Kształt tensora: (1, 150, 150, 3).\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas wyświetlić zdjęcie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu wyciągnięcia map cech, którym chcemy się przyjrzeć utworzymy model Keras, który będzie przyjmował na wejściu wsady składające się z obrazów i generował na swoim wyjściu aktywacje wszystkich warstw konwolucyjnych i warstw łączących. W tym celu skorzystamy z klasy Model pakietu Keras. Instancję modelu utworzymy korzystając z dwóch argumentów: tensora wejściowego (lub listy tensorów wejściowych) i tensora wyjściowego (lub listy tensorów wyjściowych). Utworzona w ten sposób klasa jest modelem Keras, który podobnie do opisanych wcześniej modeli sekwencyjnych (Sequential) mapuje określone dane wejściowe do określonych danych wyjściowych. Klasę Model od klasy Sequential odróżnia to, że umożliwia ona tworzenie modeli z wieloma wyjściami. Więcej informacji na temat klasy Model znajdziesz w podrozdziale 7.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "# Wyciąga dane wyjściowe ośmiu górnych warstw.\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "# Tworzy model zwracający dane wyjściowe generowane w wyniku przekazania określonych danych wejściowych.\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ten po skierowaniu do jego wejścia obrazu zwraca wartości aktywacji warstwy oryginalnego modelu. To pierwszy przykład modelu zwracającego wiele danych wyjściowych, który został zaprezentowany w tej książce. Dotychczas przedstawiałem modele, które przyjmowały dokładnie jeden obiekt wejściowy i zwracały jeden obiekt wyjściowy, ale ogólnie rzecz biorąc, model może przyjmować dowolną liczbę obiektów wejściowych i generować dowolną liczbę obiektów wyjściowych. Opisywany aktualnie model przyjmuje jeden obiekt wejściowy i generuje osiem obiektów wyjściowych (po jednym dla każdej warstwy aktywacji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zwraca listę pięciu tablic Numpy: po jednej tablicy dla każdej warstwy aktywacji.\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oto przykład aktywacji pierwszej warstwy konwolucyjnej wygenerowanej na podstawie wybranego przez nas zdjęcia kota:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to mapa cech o rozmiarach 148x148 składająca się z 32 kanałów. Przedstawmy w formie graficznej czwarty kanał aktywacji pierwszej warstwy oryginalnego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kanał ten wydaje się kodować informacje o wykryciu linii ukośnych. Spróbujmy przyjrzeć się siódmemu kanałowi — pamiętaj o tym, że we własnym kodzie informacje, które zawiera mój siódmy kanał, mogą znajdować się w innym kanale, ponieważ filtry tworzone przez warstwy konwolucyjne nie mają charakteru deterministycznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa ta wydaje się wykrywać „jasne zielone kropki” — jest to dobry sposób na zakodowanie wizerunku kocich oczu. Wygenerujmy kompletną wizualizację wszystkich warstw aktywacji naszej sieci. Wyciągniemy dane każdego kanału ośmiu map aktywacji i przedstawimy je na wykresie majacym formę jednego dużego zbioru obrazów (kanały zostaną umieszczone obok siebie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Na wykresie umieścimy również nazwy warstw.\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Wyświetla mapy cech.\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # Liczba cech w mapie.\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # Mapa cech ma kształt (1, size, size, n_features).\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Tworzy kafelki kanałów aktywacji w tej macierzy.\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # Tworzy poziomą siatkę składajacą się z filtrów.\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Przetwarzanie cechy w celu wygenerowania czytelnej wizualizacji.\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Wyświetla siatkę.\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zauważyć tu kilka rzeczy:\n",
    "\n",
    "* Pierwsza warstwa wykrywa różne krawędzie. Na tym etapie aktywacje zachowują prawie wszystkie informacje obecne w początkowym obrazie.\n",
    "* Kolejne warstwy aktywacji stają się coraz bardziej abstrakcyjne i trudniej je zinterpretować graficznie. Mogą one zaczynać kodować wysokopoziomowe koncepcje takie jak „kocie ucho” i „kocie oko”. Wyższe reprezentacje danych zawierają coraz mnie informacji dotyczących wizualnej zawartości obrazu i jednocześnie coraz więcej informacji związanych z klasą obrazu.\n",
    "* Wraz z głębią warstw maleje ich gęstość: wszystkie filtry pierwszej warstwy są aktywowane obrazem wejściowym, ale w kolejnych warstwach coraz więcej filtrów jest pustych. Oznacza to w obrazie brak wzorca zakodowanego przez dany filtr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Właśnie odkryliśmy ważną cechę reprezentacji uczonych przez głębokie sieci neuronowe: wraz ze wzrostem głębokości warstwy rozpoznawane przez nią cechy stają się coraz bardziej abstrakcyjne. Aktywacje wyższych warstw zawierają coraz mniej informacji o obserwacji wejściowej, jednocześnie zawierają coraz więcej informacji o celu (klasie, do której należy obraz). Głęboka sieć neuronowa pełni funkcję potoku oczyszczającego informację — na wejściu tego potoku podawany jest obraz (w tym przypadku jest to obraz w formacie RGB), który w wyniku powtarzania przekształceń jest odzierany ze zbędnych informacji (np. wizualnych parametrów danego zdjęcia), co prowadzi do wzmocnienia przydatnych informacji, na podstawie których można ustalić klasę, do której należy obraz.\n",
    "\n",
    "W taki sam sposób świat postrzegają ludzie i zwierzęta: po obserwowaniu jakiejś sceny przez kilka sekund człowiek zapamiętuje abstrakcyjne obiekty, które były w niej obecne (takie jak rower lub drzewo), ale nie pamięta konkretnego wyglądu tych obiektów. Spróbuj narysować rower z pamięci. Najprawdopodobniej będzie to problematyczne pomimo tego, że z pewnością zdarzyło Ci się w życiu widzieć rowery wielokrotnie. Spróbuj to zrobić teraz. Nasz mózg naprawdę działa, ucząc się abstrakcji bodźców wizualnych i odzierając je ze zbędnych szczegółów. Przez to zjawisko dość trudno jest zapamiętać detale wyglądu otaczających nas rzeczy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
